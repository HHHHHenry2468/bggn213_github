---
title: "Class 7: Maching Learning I"
author: "Henry Li (PID: 16354124)"
format: pdf
---

Today we begin our exploration of some "classical" machine learning approaches. We will start with clustering:

Let's first make up some data to cluster where we know what the answer should be.

```{r}
# rnorm() generates random numbers from a normal distribution
# Arguments: mean, sd, n
# Required arguments: n (mean = 0, sd = 1)
hist(rnorm(114514))
```

```{r}
x <-  c(rnorm(100, -3, 1), rnorm(200, 2, 1))
hist(x)
```

```{r}
# rev() reverses the order of a vector
y <- rev(x)
b <- cbind(x,y)
head(b)
tail(b)
```

A peek at a few rows of the data:
```{r}
plot(b)
```

The main function in "base" R for K-means clustering is called `kmeans()`.

```{r}
kmeans(b, centers = 2)
```

> Q. How big are the clusters (i.e. their sizes)?

```{r}
k <- kmeans(b, centers = 3)
k$size
```

> Q. What clusters do my data point resides in

```{r}
k$cluster
```

> Q. Make a plot of our data colored by cluster assignment - i.e. Make a result figure.

```{r}
# Assign cluster 1 cyan, cluster 2 violet, cluster 3 red
plot(b, col = c("cyan", "violet", "red")[k$cluster], pch = 19)
points(k$centers, col = c("purple", "red", "blue"), pch = 8, cex = 2)
```

> Q. How about asking for more clusters?

```{r}
k <- kmeans(b, centers = 5)
plot(b, col = c("cyan", "violet", "red", "green", "orange")[k$cluster], pch = 19)
points(k$centers, col = c("purple", "red", "blue", "black", "yellow"), pch = 8, cex = 2)
```

> Q. Run kmeans with centers (i.e. values of k) equal 1 to 6, and store the tot.withinss.

```{r}
getSS <- function(k) {
  km <- kmeans(b, centers = k)
  return(km$tot.withinss)
}
ss <- sapply(1:6, getSS)
# ss is a vector of tot.within, it stands for "total within-cluster sum of squares", which symbolizes how tightly the data points in each cluster are grouped together. The larger ss is, the more dispersed the clusters are.
plot(ss, typ='b')
```

## Hierarchical Clustering

The main function in "base" R for this is called `hclust()`.

```{r}
# The argumetn to hclust(), d, is a distance matrix, which can be generated by the dist() function. dist() computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix.
d <- dist(b)
hc <- hclust(d)
hc
```

```{r}
#Dendrogram
plot(hc)
abline(h = 8, col = "red")
```

To obtain clusters from our `hclust` result object **hc**, we "cut" the tree to yield different sub-branches. For this, we use the `cutree()` function.

```{r}
# 1. Cut by desired cluster number
grps1 <- cutree(hc, k = 3)
plot(b, col = c("cyan", "violet", "red")[grps1], pch = 19)
```

```{r}
# 2. Cut by desired height
grps2 <- cutree(hc, h =10)
plot(b, col = c("cyan", "violet", "red")[grps2], pch = 19)
```

## Principal Component Analysis (PCA)

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
# Import data
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
dim(x)
head(x)
```

Set the rownames() to the first column and then removes the troublesome first column (with the -1 column index)

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
dim(x)
```
An alternative approach:
```{r}
x <- read.csv(url, row.names=1)
head(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I would say the second one is easier as we directly modify the row names when reading the csv file. The first approach is more flexible as we can do more operations on the data frame before setting the row names.As for robustness, I think the second one is better, because the first one is constantly overwriting x with removing the first row and make it the name of the rows.

```{r}
# Using base R
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
# Make the barplot filled rather than dodged.
barplot(as.matrix(x), beside = F, col=rainbow(nrow(x)))
```
Looks like people in Wales eat more :)

## Pair plots and heatmaps

Scatterplot matrices can be useful for relatively small datasets like this one. Let's have a look.

> Q5: We can use the pairs() function to generate all pairwise plots for our countries. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

```{r}
# Heatmap
library("pheatmap")
pheatmap( as.matrix(x) )
```
> Q6. Based on the pairs and heatmap figures, which countries cluster together and what does this suggest about their food consumption patterns? Can you easily tell what the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

Based on the pairs and heatmap figures, it appears that England and Wales cluster together, then Scotland, then Northern Ireland.

## PCA to the rescue

The main function in "base" R for PCA is called `prcomp()`. As we want to do PCA on the food data for the different countries we will want the foods in the columns.

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```

Our result object is called `pca` and it has a `$x` component that we will look at first

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
library(ggplot2)
ggplot(pca$x) +
  aes(PC1, PC2, label = rownames(pca$x)) +
  geom_point(size=8, color=c('orange', 'red', 'blue', 'darkgreen'), pch = 18) +
  geom_text()
```

Another major result out of PCA is the so-called "variable loadings" or `$rotation`. It tells us how the original variables (foods) contribute to the PCs to the new axis.

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
ggplot(pca$rotation) +
  aes(x = PC1, 
      y = reorder(rownames(pca$rotation), PC1)) +
  geom_col(fill = "steelblue") +
  xlab("PC1 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))
```

> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
ggplot(pca$rotation) +
  aes(x = PC2, 
      y = reorder(rownames(pca$rotation), PC2)) +
  geom_col(fill = "royalblue") +
  xlab("PC2 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))
```