---
title: "Class08"
author: "Henry Li (A16354124)"
format: pdf
toc: true
---

## Background

The goal of this mini-project is to explore a complete analysis using the unsupervised learning techniques covered in class. Extend what was learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.

## Data import

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

Make sure we do not include sample ID or diagnosis columns in the data that we analyze below.

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[, -1]
dim(wisc.data)
```

## Exploratory data analysis

> Q1. How many observations are in this dataset?

There are 569 observations in this dataset. This can be seen at dim().

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

There are 212 observations with a malignant diagnosis.

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
mean_vars <- grep("_mean$", colnames(wisc.data), value = TRUE)
length(mean_vars)
```

There are 10 variables/features in the data that are suffixed with _mean.

## PCA analysis

What does scale and center in `prcomp(x, scale = F, center = F)` mean?

In general we want to scale and center our data prior to PCA to ensure that each feature contributes equally to the analysis, precventing variables with larger scales from dominating the principal components.

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE, center = TRUE)
a <- summary(wisc.pr)
summary(wisc.pr)
```

Let's make our main result figure - the "PC plot" or the "score plot", "ordination plot".

```{r}
library(ggplot2)

ggplot(wisc.pr$x) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

```{r}
explained_variance <- summary(wisc.pr)$importance[2, ]
explained_variance[1]
```
44.27% of the original variance is captured by the first principal component (PC1).

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
cumulative_variance <- summary(wisc.pr)$importance[3, ]
num_pcs_70 <- which(cumulative_variance >= 0.7)[1]
num_pcs_70
```

There are 3 principal components (PCs) required to describe at least 70% of the original variance in the data.

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
num_pcs_90 <- which(cumulative_variance >= 0.9)[1]
num_pcs_90
```

There are 7 principal components (PCs) required to describe at least 90% of the original variance in the data.

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```
Some vectors stood out and appears to be outstanding from the cluster. It is very difficult to understand.

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[, c(1, 3)], col = diagnosis, pch = 19,
     xlab = "PC1", ylab = "PC3", main = "PCA Plot: PC1 vs PC3")
```

I noticed that the separation between the two diagnoses is less clear in the PC1 vs PC3 plot compared to the PC1 vs PC2 plot.

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

```{r}
loading_vector_pc1 <- wisc.pr$rotation[, 1]
loading_concave_points_mean <- loading_vector_pc1["concave.points_mean"]
loading_concave_points_mean
```

## Hierarchical clustering

>Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
wisc.dist <- dist(wisc.pr$x)
wisc.hclust <- hclust(wisc.dist)

k <- 4
h <- wisc.hclust$height[length(wisc.hclust$height) - (k - 1)]

plot(wisc.hclust)
abline(h = h, col = "red", lwd = 2, lty = 2)
```

Another option

```{r}
# make std = 1
d <- dist(scale(wisc.data))
h <- hclust(d)
k <- 4
j <- wisc.hclust$height[length(h$height) - (k - 1)]
plot(h)
abline(h = j, col = "red", lwd = 2, lty = 2)
```

# Combining PCA and hierarchical clustering

```{r}
d <- dist(wisc.pr$x[, 1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot(wisc.pr.hclust)
abline(h = 60, col = "red", lwd = 2)
```

Get my cluster membership vector
```{r}
cutree(wisc.pr.hclust, h = 70)
```

>Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

Make a wee "cross-table"

```{r}
table(cutree(wisc.pr.hclust, h = 70), diagnosis)
```

Sensitivity: TP/(TP+FN) = 190/(190+22) = 0.8962264